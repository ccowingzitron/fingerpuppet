#!/usr/bin/env python

import fingerpuppet
import tables

# General Description of Raw Data Storage

# All data used by fingerpuppet will be stored in HDF5 files called 'pets'. The various flavours of pet serve different purposes:
# 'giraffes' permanently store analysis-generating data, and are the only pet the user needs to keep in order to re-analyse an experiment.
# 'lemurs' store annotation databases used to describe the genomic structures the user is interested in. lemurs are optional, in that 
# the actual data analysis does not require them, and the output from fingerpuppet is in the form of standard file types that can be 
# integrated with third-party annotation software. fingerpupppet can automaticaly generate some lemurs from user-provided annotation
# files, but can also be manually built by the user. lemurs can be reused, as they are not specific to experiments, though a giraffe may
# optionally contain a lemur, so as to enable easy transfer of all the data for an analysis in a single file. 'chameleons' hold the 
# intermdiate data generated by fingerpuppet as it runs GAMS. This includes modelling commands, read depth data, sample relationships,
# and the constraints that define the models, as well as the raw GAMS output, be it estimates of gene expression or statistics for
# alternative splicing. Once again, a chameleon may be incorporated into a giraffe, though in general chameleons will be temporary 
# files which will not be saved, as they can be regenerated by fingerpuppet using a giraffe. On the other hand, the permanent output file
# of fingerpuppet is called a 'waterbear'. While the fingerpuppet generates output files for use with third-party software, i.e. Excel
# workbooks and text reports, for its own purposes and for quick access to analytical output, the data will be stored in a waterbear.
# The user may transfer this file rather than the giraffe, if they wish to share the results of an experiment but not the raw data. The
# waterbear can also optionally store annotation, but unlike a lemur, which stores annotation for all genomic structures using a database
# structure, the waterbear stores annotation on a structure-by-structure basis immediately alongside the statistical database. This is
# because a lemur stores large amounts of data for annotation work for any experiment, whereas the waterbear serves to enable easy
# exploration of the output data for a single experiment. Finally, fingerpuppet itself may be stored in a 'raccoon'. raccoons are HDF5
# files, just like other pets, but they store software rather than data. The purpose is not to run fingerpuppet from a raccoon; indeed,
# fingerpuppet must be extracted from the raccoon, and then compiled by the user, in order to be run. Rather, the raccoon is designed
# to be incorporated into a giraffe. This enables the user to transfer a single file to another user, which contains everything not
# third-party that is required for a fingerpuppet analysis of the data. So if another had the necessary setup, specifically python, 
# numpy, scipy, PyTables, and GAMS, all they would need is a single giraffe pet to run an analysis. Indeed, fingerpuppet is optionally
# distributed as raccoon-containing giraffe, which hold both the software itself and example data on which to test the model.

# Giraffes: The Pet Everyone Needs.

# The experiment-specific data required to run fingerpuppet will be stored within one HDF5 file, called a giraffe. The data is broken
# into the tables described below: 1) genomic structures of interest, e.g. genes or exons, that the experimental data may be aligned to,
# which may be from public databases or defined by the user; these structures serve as the top two tiers of sets defined in the GAMS
# model, genes/transcripts being the top-tier, and exons being the second tier; 2) genomic windows defined by the user which are the 
# basic units against which RNA-Seq data are calculated, and which serve within the GAMS model as the lowest level of set, the one with
# reference to which the actual read-depth parameters are referred; 3) the actual read depth data, calculated with respect to the windows
# defined above. The giraffe will be created once at the beginning of the analysis of the experiment. The three tables can be added to
# repeatedly by the user, so as to enable an evolution of the targets of analysis. There will be a single window table and a single 
# genomic structures table within the giraffe; they will simply be appended to when the user selects new windows and subsets to analyse.
# On the other hand, there will be multiple read depth tables. One will be created for each user-selection of window size and sample
# subset; the reason will be explained below. There is also a fourth data collection which describes various details of the 
# window/structure subsets defined by the user, as well as read depth summaries, enabling quick and repeatable reference to and 
# overviewing of any given analytical design. These four data structures are necessary for the running of fingerpuppet, and can only
# be generated from raw BAM files of genomic alignment; furthermore, generating such data structures takes time, thus making it worth
# the user's while to do it only once.

# The Optional Data Structures, Or, A Giraffe's Best Friends

# There are three optional data structures the user can store within a giraffe or as independent pets: annotation, GAMS data, and 
# fingerpuppet. The annotation will be contained within a lemur, and is entirely optional. In fact, while fingerpuppet can annotate its
# results using a lemur, it does not run any actual analyses using the data. The annotation may include whatever databases the user 
# chooses to apply to describe the genomic structures analysed, but the options are currently limited. This data will be held in a number
# of different group/table structures, depending upon the structure of the annotation. This has two significant caveats: experimentality
# and size. This annotation storage is experimental basically because the huge variety of annotation database structures and file formats
# means that creating databases to store such data is difficult. As such, when converting from public databases to our HDF5 files, some
# data may be lost or mis-translated. The size issue is equally important: annotation gets huge very quickly. While it is nice to have
# all your data in one file, if that file is 10GB in size, it becomes unwieldy. There is no way around this with the experimental data:
# a giraffe is a big animal, and so is an RNA-Seq experiment, period. Initally, support will be provided for the user to import
# two different annotation formats into their lemurs: GTF and BED. These two formats are the most widely used and easiest to convert.
# The second optional data structure are the actual GAMS data used in running GAMS. These come in two flavours: GMS and GDX. The GMS
# data holds the various execution information for a GAMS model: the modelling commands, control flow constructs, output commands, etc.
# The GDX holds the sets, parameters, constraints, and equations that define the model itself. The GMS data will be small, as
# the same GMS content will be reused for each gene/transcript/exon analysed. The GDX data, on the other hand, will be generated anew for
# each genomic entity analysed (gene, etc.), as it contains necessary details as to the physical structure of the entity, the read depth
# at each window defined for that entity, as well as the specific constaints on the variables that determine both the statistical function
# of the GAMS model (estimation or testing) as well as the experimental design (samples and their relationships). This storage of GAMS
# data is optional because fingerpuppet can regenerated from the required data in the HDF5 file. That being said, generating these data
# for, say, all the genes in the human genome, does take considerable time, thus perhaps meriting their storage. In fact, these data will
# always be stored in a chameleon when running fingerpuppet, so incorporating them into a user's giraffe is just a matter of merging the
# two. The user can optionally have the chameleon produce actual GAMS format GMS and GDX files, so the the model may be executed and 
# manipulated independently of fingerpuppet. This last point is important: an entire GAMS model may be stored in a single GMS text file,
# enabling the user to completely redesign the model. The zookeeper (me) encourages you to do this, as any model can be improved upon,
# and a fundamental philosophy of the design of fingerpuppet is customization by the user. The final optional data structure is 
# fingerpuppet itself, that is, a pet raccoon. A raccoon simply holds the textual data required to write .py files for fingerpuppet.
# It is inefficient storage, and while technically human-readable, is rather incoherent, since HDF5 is not designed to store text files.
# It serves only to enable an advanced user, using PyTables, to extract runnable code from a giraffe, and would be essentialy useless
# without either BAM files or a giraffe to go with it; as such, raccoons are usually distributed as part of a giraffe. Note that there is
# no way to make an HDF5 file 'executable', that is, I cannot automate the extraction of fingerpuppet from a raccoon, so if you don't
# really know what you're doing with PyTables, it's best to just use the available .py files directly.

# Detailed Description of Group/Table Organization of A Giraffe

# The giraffe will be organized into three groups: 'summary_data', 'locus_data', 'relationship_data', and 'read_data'. The 'detail_data' group will have three
# leaves. The 'window_summary' table will hold details of each window size selection the user has performed. This includes: window size,
# number of windows created, data and time created, various statistics related to the windows, a user-defined ID for the window selection,
# and the rows of the actual windows within the 'windows_table'. The user will be able to call up the windows by their ID. The other 
# subsetting table, 'structure_summary', will hold analogous information for the genomic structures subsets defined by the user. The
# third table, 'read_depth_summary' is different in design: it hold summary information for the actual read depths calculatd by the user.
# The first row will always describe the entire data set, not broken into windows. The subsequent rows will give information about each 
# read depth analysis performed by the user. The data provided here will include statistics about the read depth (mean, standard 
# deviation, etc.), as well as meta-data describing this data selection. The 'locus_data' group has two leaves: 'windows_table' and 
# 'structure_table'. Their content forms the core of the database lookup capabilities for the HDF5 file, in that they hold the actual 
# loci to which the read depth data are referenced; the 'structure_table' also contains meta-data for each structure desribed. Whereas 
# each user-defined selection of genomic structure and window size was described in toto by a single row in their respective summary
# table, the 'locus_data' tables have a row for each individual locus defined by the user, and all loci of a given type (structure vs. 
# window) will be held in a single table. The tables will be held in the fixed order in which the subsets are defined, thus enabling
# the summary tables to reference their respective subsets simply by start and end row. This also enables the read depth data to be
# referenced to the locus data by the row number, rather than to the actual locus; this will save a lot of disk space and speed up 
# the lookup process. The 'read_data' group will hold all the read depth tables the user calculates; obviously there will be a 
# variable number of these. One such table will be created for each user selection of a combination of window size and sample subset. The
# reason for subsetting by samples is that it enables fixed numbers of columns per table, which saves size and speeds up data access.
# The reason for subsetting window size is two-fold and less obvious: first, it enables quicker summary statistic calculation for each
# selection of window size, and second (and more importantly), it enables the user to pull up windows of a given size for a given subset
# of genomic structures quickly and with the certainty that no windows will overlap; this would require the storage of far more redundant
# data if all window sizes were stored together. The rationale for the partitioning of read depths based upon just these two variables is
# that this is how the user is envisioned to organize their analysis, because it is the most natural given the structure of the model
# of RNA-Seq used in this package. The user will have the option, though, of creating custom read depth tables for use with the model.

# Speed and Storage Considerations

# All tables will be indexed for fast searching/lookup; this will be possible because no tables will have nested structures, and they
# will all be sorted. There will also be links between the various tables, so as to allow simultaneous retrieval of genomic structure,
# window locus, and read depth, starting from any one table. The goal is a balance between three things: speed of data access, achieved 
# via indexing and and linearity of data structures, conservation of disk space, achieved via minimal redundancy of data storage, and
# easy of experiment organization and data exploration by the user. With this organization, the user could open their giraffe in a 
# viewer such as ViTables, visually or textually select a subset of genomic structures, and view all read depth data calculated for 
# these structures at all resolutions and samples. The user can then immediately pull out these data and run them through the GAMS model,
# allowing for highly customized, convenient analyses. Furthermore, all the data for the experiment, including the raw read depths, the 
# various structures analysed, the meta-data for the analyses, and optionally the annotation of the structures and the individual GAMS
# models used, will be at hand in a single file for the user to move and share; a single file which will be efficient in storage, quick
# in data access, comprehensive in content, and easy to explore. 

# Indvidual giraffe tables

# Here are the definitions of the PyTables classes used in a giraffe. All tables are based upon tables.IsDescription. There are four
# base classes: 'locus_table', 'detail_table', 'read_depth_table', 'relationship_table', and two child classes: 'structure_table' and 'read_depth_summary_table'.
# locus_table hold just genomic loci without any annotation; the only direct implementation of this will be to hold genomic windows within
# the 'window_table', since only windows don't need to be desribed (they serve solely to map read depth to genomic strucures). 
# structure_table is a child of locus_table, to which is added an ID column, a source column, a level column, and a coding_state column. 
# The ID and source are clear; level can be any of 'gene', 'transcript', or 'exon', and coding state may be either 'coding', 'non_coding', 
# or 'full' for genes and transcripts, and 'coding', 'non_coding', or 'virtual' for exons. These extra columns enable rapid lookup of 
# genomic structures. read_depth_table is very minimal: one column holds the row number of the corresponding window within the 
# window_table, and one column holds the actual read depth. The detail_table class has columns for date of creation, source, number of 
# entries, and average/max/min/stdev entry size. This will be the summary table for both genomic structures and windows. The 
# read_depth_summary_table will add to this statistics for read depth: mean, max, min, stdev, etc.

class locus_table(tables.IsDescription):

	contig = tables.StringCol(32)
	start = tables.UInt64Col()
	end = tables.UInt64Col()
	strand = tables.BoolCol()
	
class detail_table(tables.IsDescription):

	creation_date = tables.StringCol(32)
	source = tables.StringCol(32)
	entries = tables.UInt64Col()
	mean_size = tables.Float64Col()
	max_size = tables.Float64Col()
	min_size = tables.Float64Col()
	stdev_size = tables.Float64Col()
	
class read_depth_table(tables.IsDescription):

	window_row = tables.UInt64Col()
	read_depth = tables.UInt16Col()
	
class relationship_table(tables.IsDescription):

	parent_id = tables.StringCol(64)
	parent_level = tables.StringCol(10)
	child_id = tables.StringCol(64)
	child_level = tables.StringCol(10)
	
class structure_table(locus_table):

	id = tables.StringCol(64)
	source = tables.StringCol(32)
	level = tables.StringCol(10)
	coding_state = tables.StringCol(10)
	
class read_depth_summary_table(detail_table):

	total_depth = tables.UInt64Col()
	mean_depth = tables.UInt64Col()
	max_depth = tables.UInt64Col()
	min_depth = tables.UInt64Col()
	stdev_depth = tables.UInt64Col()